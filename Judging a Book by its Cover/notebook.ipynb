{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T07:19:28.022505Z",
     "iopub.status.busy": "2024-09-29T07:19:28.022069Z",
     "iopub.status.idle": "2024-09-29T07:19:28.027345Z",
     "shell.execute_reply": "2024-09-29T07:19:28.026304Z",
     "shell.execute_reply.started": "2024-09-29T07:19:28.022470Z"
    }
   },
   "source": [
    "## Dataset is Exremely Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T09:12:59.084576Z",
     "iopub.status.busy": "2024-11-11T09:12:59.083571Z",
     "iopub.status.idle": "2024-11-11T09:12:59.340282Z",
     "shell.execute_reply": "2024-11-11T09:12:59.339346Z",
     "shell.execute_reply.started": "2024-11-11T09:12:59.084526Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csv_file_y='/kaggle/input/col774-2022/train_y.csv'\n",
    "dataY = pd.read_csv(csv_file_y)\n",
    "# print(dataY)\n",
    "\n",
    "# l=[0 for i in range(30)]\n",
    "# for i in dataY['Genre']:\n",
    "#     l[i]+=1\n",
    "# print(l)\n",
    "# Plotting the histogram\n",
    "dataY['Genre'].plot(kind='hist', bins=30, rwidth=0.8, color='skyblue')\n",
    "\n",
    "# Display the plot\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of column_name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T09:13:13.793795Z",
     "iopub.status.busy": "2024-11-11T09:13:13.793368Z",
     "iopub.status.idle": "2024-11-11T09:52:12.966978Z",
     "shell.execute_reply": "2024-11-11T09:52:12.960146Z",
     "shell.execute_reply.started": "2024-11-11T09:13:13.793761Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd90072b9ad04de8b3ac52fd79a4a5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df08bebda88a470aa5c93076cb514cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eaa451cc7fa42f29d65bacc5a9e5898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2dd573ffa3d4143a45e6f3ec20f26af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: \"https://github.com/pytorch/vision/archive/v0.10.0.zip\" to /root/.cache/torch/hub/v0.10.0.zip\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03f02d08e9e411197e793a5f01c4217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1069/1069 [07:20<00:00,  2.43it/s, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished with loss: 1.3197673559188843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 1069/1069 [02:25<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Confusion Matrix:\n",
      "[[1051    1    0    1    2    0   13    0    5    1    5    2   15    0\n",
      "     1    0    5    0    7    1    2    0    1   12    0    2    6    0\n",
      "     0    1]\n",
      " [   6  838    1    5    4    4   19   10    5    3    6    7   30    3\n",
      "     4    3    0    0    1    7   25    3    7   18    0   18  118    4\n",
      "     3    3]\n",
      " [   2    8  891   19   10    1    2    8    2    0    8    2    3    1\n",
      "     1    0    0    1    0    9    2   12    1    0   10   18    2   43\n",
      "     1   91]\n",
      " [   3    2    6  909    9    1    0    4    1    1    3    1    6    4\n",
      "     3    0    0    0    0    3   25    7    1    5   16   44   35   13\n",
      "     5    9]\n",
      " [   0    3   25    2  873    2    1   28   12    1   52    1   10    4\n",
      "    15    1    6    6    0   13   13    4    2    7    3   14    4   10\n",
      "     0   21]\n",
      " [  21   12    0    0    1  797    4   21    5   51   45    3   94    1\n",
      "    28    0   16    1   18    3    0    0    1   18    1    2    5    0\n",
      "     0    5]\n",
      " [  66   13    2    3    1    6  812    2    7    1    1    7   89   19\n",
      "     4    6    6    0    7    6    6    2    3   21    0    2   27    1\n",
      "     9    3]\n",
      " [   3    6    5    2   12    9    0  819    8   14  118    4    4    2\n",
      "    24    5    9    1    3   23    4    6    4    5    0   24    3    4\n",
      "     1   10]\n",
      " [  25    6    0    6   27    4   14    7  810    3   11   21   18   19\n",
      "    19    2   12    7   10    3   17    1    9   23    5   15   28    6\n",
      "    20    1]\n",
      " [   7    6    1    2   12   62    3   63    3  670  147   16   23    1\n",
      "    20    1    0    9    0    5   14   14   31   11    0    1   14    0\n",
      "     5    3]\n",
      " [   8    0    4    2   28   13    0   54   14   23  846    3   14    1\n",
      "    21    4    5   14    3    9    8    5    3   23    0    4   12    0\n",
      "     0    8]\n",
      " [   5   11    4    5    1    2   18    1    4    4    9  898   28    6\n",
      "     2    1    1    1    2   37   26    6    2    6    1   17    6    0\n",
      "    24    7]\n",
      " [  20   24    2    2    2    7   21    1    4    0    5    8  985   12\n",
      "    11    3    1    3    3   13    2    0    1   13    2   15    4    0\n",
      "     6    5]\n",
      " [   1    2    1    4   23    0    5    0   11    0    1    2   23  859\n",
      "    47    0    8   11    1   52    3    3    0    7    2   10    9    0\n",
      "    14    8]\n",
      " [  23    4    2    4   46    6    1   17   48    2   42    3   58   20\n",
      "   623    0   20   11    4   27   21    1    0   48    2    9   35   10\n",
      "     1   10]\n",
      " [   1    3    0    1    2    0    1    1    5    0    3    2    1    0\n",
      "     0 1108    0    1    0    1    2    4    1    0    0    3    8    0\n",
      "     1    0]\n",
      " [  15    1    0    0    4    9   11    8    3    1    4    1    6   12\n",
      "    39    0  884    2    4   20    4    0    0   99    1    0    2    0\n",
      "     1    5]\n",
      " [   1    2   14    4  233    0    2    8    8    0   60    2   19   12\n",
      "    42    4   15  607    1   10   10   10    2    7    4   14    9    7\n",
      "     3   11]\n",
      " [  20    0    0    1    0   13   25    2   39    0    1    1   26    1\n",
      "     1    0   14    0  981    3    1    0    0    9    0    3    3    1\n",
      "     1    0]\n",
      " [   4    3    9    5   24    1    4   14    8    3   25   41   29   42\n",
      "    37    0   13    9    0  699   44   10    4    9    2   20   32    8\n",
      "    10   47]\n",
      " [  30   32    7   26    8    1    8   12   21    7   24   13   40    1\n",
      "     9    9    5    1    3   37  659   10   20   17   20   38   62   12\n",
      "     6   11]\n",
      " [  18   16   70   38   12    2    8   38   33   12   29   33   46   12\n",
      "    27    1    5   10    5   39   29  352   10    2   19  105   33   20\n",
      "     3   73]\n",
      " [   3   16    0    3    0    0    3    3    0   40    2    0   10    2\n",
      "     0    0    0    0    0    3    1    0 1008    6    0    5    4    1\n",
      "     5    1]\n",
      " [  56    2    0    1    5    6   29    2   11    0   23    5   14    5\n",
      "    18    2   15    2   16    8    4    0    1  916    1    1   15    0\n",
      "     0    0]\n",
      " [  14    6   66   45    5    0    2    0    3    0    2    1   14    2\n",
      "     4    0    0    4    0    4   12   30    2    1  767   33    2   41\n",
      "     1  111]\n",
      " [  19   15    4   29   14    1   12    5    6    0    5   20   35   11\n",
      "     2    1    2    5    4    9   22   31    8    3    8  813   25    6\n",
      "     5   12]\n",
      " [  44   38    7   19    7    1   42    2    9    0    4   13   14    7\n",
      "     4    9    0    2    0   16   47    3    0   12    0   12  804    1\n",
      "    11    7]\n",
      " [   2   12   78   20   35    1    0   10   35    2   20    5   20   17\n",
      "    31    0    2   12    0   46   36   23    6    3   21   65   20  456\n",
      "     5  185]\n",
      " [   1    2    0    1    3    0    8    0   18    0    1   42   16   33\n",
      "     1    8    1    0    0   22    2    3   12    4    1   11   25    1\n",
      "   945    8]\n",
      " [   2    1   42   17    5    0    0    2    2    0    4    1    4    1\n",
      "     2    0    3    0    0   10   12    3    1    0   11    5    3   46\n",
      "     3  973]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.93      0.81      1134\n",
      "           1       0.77      0.73      0.75      1155\n",
      "           2       0.72      0.78      0.75      1148\n",
      "           3       0.77      0.81      0.79      1116\n",
      "           4       0.62      0.77      0.69      1133\n",
      "           5       0.84      0.69      0.76      1153\n",
      "           6       0.76      0.72      0.74      1132\n",
      "           7       0.72      0.72      0.72      1132\n",
      "           8       0.71      0.70      0.71      1149\n",
      "           9       0.80      0.59      0.68      1144\n",
      "          10       0.56      0.75      0.64      1129\n",
      "          11       0.78      0.79      0.78      1135\n",
      "          12       0.58      0.84      0.69      1175\n",
      "          13       0.77      0.78      0.77      1107\n",
      "          14       0.60      0.57      0.58      1098\n",
      "          15       0.95      0.96      0.96      1149\n",
      "          16       0.84      0.78      0.81      1136\n",
      "          17       0.84      0.54      0.66      1121\n",
      "          18       0.91      0.86      0.88      1146\n",
      "          19       0.61      0.60      0.61      1156\n",
      "          20       0.63      0.57      0.60      1149\n",
      "          21       0.65      0.32      0.43      1100\n",
      "          22       0.88      0.90      0.89      1116\n",
      "          23       0.70      0.79      0.74      1158\n",
      "          24       0.86      0.65      0.74      1172\n",
      "          25       0.61      0.72      0.66      1132\n",
      "          26       0.59      0.71      0.65      1135\n",
      "          27       0.66      0.39      0.49      1168\n",
      "          28       0.87      0.81      0.84      1169\n",
      "          29       0.60      0.84      0.70      1153\n",
      "\n",
      "    accuracy                           0.72     34200\n",
      "   macro avg       0.73      0.72      0.72     34200\n",
      "weighted avg       0.73      0.72      0.72     34200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 179/179 [00:24<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Metrics:\n",
      "Confusion Matrix:\n",
      "[[137   1   0   0   0   1   8   0   3   0   0   0   1   0   0   0   0   0\n",
      "    0   1   1   1   0   4   0   0   2   1   0   1]\n",
      " [  7 116   0   0   0   0  10   3   0   0   1   4   7   0   1   0   0   0\n",
      "    0   1   2   1   2   4   0   3  23   1   0   3]\n",
      " [  0   2 101   9   4   0   1   1   0   0   7   3   2   0   0   0   0   0\n",
      "    0   5   2   5   1   1   3   4   2  12   2  17]\n",
      " [  3   2   5 147   2   1   1   1   2   0   1   0   4   1   1   0   0   0\n",
      "    0   2   7   3   2   0   5  16   7   2   0   1]\n",
      " [  1   3   3   3 131   0   0   4   2   1  13   0   3   0   2   0   1   1\n",
      "    0   2   1   0   2   1   2   3   0   3   2   0]\n",
      " [  0   0   0   0   0  97   0   2   1  10   3   2  15   0   5   0   1   0\n",
      "    5   1   2   0   0   3   1   0   1   0   0   1]\n",
      " [ 17   3   0   2   1   1 133   0   0   0   2   3  21   2   2   0   0   0\n",
      "    2   1   1   0   1   7   0   2   5   0   1   2]\n",
      " [  0   2   1   0   2   3   0 103   2   3  18   0   2   0   6   0   1   0\n",
      "    1   7   3   2   1   4   0   5   0   0   0   2]\n",
      " [  6   2   0   4  11   2   4   3 101   1   3   5   7   5   5   0   4   1\n",
      "    0   1   5   1   2   6   4   8   4   0   3   0]\n",
      " [  4   3   2   0   2  13   2  11   1  80  32   6  12   0   4   0   0   0\n",
      "    0   2   5   3   5   5   0   1   6   0   0   0]\n",
      " [  4   1   0   1  11   2   0  12   3   6 139   0   4   1   5   1   0   2\n",
      "    0   4   1   1   0   8   0   2   4   0   0   5]\n",
      " [  0   5   2   2   0   0   6   0   3   3   2 112  10   2   0   0   0   0\n",
      "    0  11   5   1   0   1   0   8   2   0   5   5]\n",
      " [  7   5   0   1   1   4   6   0   3   0   3   4 114   5   2   0   1   0\n",
      "    1   2   3   0   3   7   1   2   5   1   3   2]\n",
      " [  1   1   0   3   1   1   4   2   6   0   1   1   9 116  10   0   5   4\n",
      "    0  15   2   0   2   1   0   6   4   0   6   2]\n",
      " [  5   2   1   0   7   5   0   5   9   0  11   0  19   3  64   1   8   2\n",
      "    0   6   3   0   0   8   1   1   7   4   0   5]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0   0   0 188   0   0\n",
      "    0   0   1   2   0   0   0   1   1   0   0   0]\n",
      " [  4   0   0   1   1   2   3   0   0   0   1   0   2   7   7   0 131   0\n",
      "    0   6   0   2   0  19   0   0   2   0   0   5]\n",
      " [  0   1   2   1  34   0   0   3   6   2   9   1   5   6  13   0   0  85\n",
      "    0   4   3   0   0   1   1   4   4   5   0   8]\n",
      " [  2   0   0   0   1   5   3   1   3   1   0   0   7   1   2   0   2   0\n",
      "  142   0   0   0   0   2   0   1   1   0   0   1]\n",
      " [  1   1   1   2   6   1   0   8   2   1   9  11   7  13   8   0   2   1\n",
      "    0  95   9   0   1   2   0   1   5   0   7   8]\n",
      " [  6   4   0   5   3   0   2   1   8   0  10   6   6   1   4   3   1   0\n",
      "    1   9  78   1   2   3   2  10  15   2   2   5]\n",
      " [  4   3  15   8   4   0   1   5   3   7   6   6   6   0   3   1   0   2\n",
      "    2  11  14  50   1   1   3  20   9   6   1  12]\n",
      " [  2   3   1   0   0   0   0   2   0  18   2   3   2   0   1   0   0   1\n",
      "    0   2   0   1 170   0   0   2   1   0   1   0]\n",
      " [ 12   1   1   0   1   0   3   1   3   0   3   2  10   4   5   0   3   0\n",
      "    1   3   1   0   0 130   0   0   4   0   0   0]\n",
      " [  3   0  16   5   3   0   4   0   1   0   0   0   4   0   2   0   0   0\n",
      "    0   4   3   5   1   0  69   8   1   6   2  21]\n",
      " [  3   7   1  11   4   1   3   2   3   0   2   3   8   2   1   0   0   3\n",
      "    1   8   3   9   2   0   3 112   7   0   0   1]\n",
      " [  6  14   2   3   3   1   7   2   1   1   4   2   8   6   1   1   0   1\n",
      "    1   6   7   4   1   2   1   1 107   1   2   2]\n",
      " [  1   2  18   6   7   1   0   6   8   0   4   0   2   8   6   0   2   2\n",
      "    0   6  10   3   0   0   5  10   4  54   3  29]\n",
      " [  2   0   0   0   1   0   5   0   7   1   1   4   2  11   2   3   1   0\n",
      "    0   3   0   0   1   1   0   0   3   0 117   0]\n",
      " [  0   0  12   1   0   0   0   0   0   0   1   1   1   1   0   0   1   0\n",
      "    0   5   2   0   0   0   4   4   0  15   0 151]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.85      0.68       162\n",
      "           1       0.63      0.61      0.62       189\n",
      "           2       0.55      0.55      0.55       184\n",
      "           3       0.68      0.68      0.68       216\n",
      "           4       0.54      0.71      0.62       184\n",
      "           5       0.69      0.65      0.67       150\n",
      "           6       0.65      0.64      0.64       209\n",
      "           7       0.58      0.61      0.60       168\n",
      "           8       0.56      0.51      0.53       198\n",
      "           9       0.59      0.40      0.48       199\n",
      "          10       0.48      0.64      0.55       217\n",
      "          11       0.63      0.61      0.62       185\n",
      "          12       0.38      0.61      0.47       186\n",
      "          13       0.59      0.57      0.58       203\n",
      "          14       0.40      0.36      0.38       177\n",
      "          15       0.95      0.97      0.96       194\n",
      "          16       0.80      0.68      0.73       193\n",
      "          17       0.81      0.43      0.56       198\n",
      "          18       0.90      0.81      0.86       175\n",
      "          19       0.43      0.47      0.45       202\n",
      "          20       0.45      0.41      0.43       190\n",
      "          21       0.53      0.25      0.33       204\n",
      "          22       0.85      0.80      0.83       212\n",
      "          23       0.59      0.69      0.64       188\n",
      "          24       0.66      0.44      0.52       158\n",
      "          25       0.48      0.56      0.51       200\n",
      "          26       0.45      0.54      0.49       198\n",
      "          27       0.48      0.27      0.35       197\n",
      "          28       0.75      0.71      0.73       165\n",
      "          29       0.52      0.76      0.62       199\n",
      "\n",
      "    accuracy                           0.59      5700\n",
      "   macro avg       0.61      0.59      0.59      5700\n",
      "weighted avg       0.60      0.59      0.59      5700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 1069/1069 [07:14<00:00,  2.46it/s, loss=1.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished with loss: 1.141708493232727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 1069/1069 [02:25<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Confusion Matrix:\n",
      "[[1046    2    0    0    1    0    7    0    2    0    7    2    6    0\n",
      "     4    0    5    0    1    0   17    1    1   14    0    1   16    0\n",
      "     0    1]\n",
      " [   0  970    6    2    4    4    7    9    2    3    0    4   11    1\n",
      "     9    2    0    3    0   10   10    1    8    4    0    5   74    3\n",
      "     1    2]\n",
      " [   0    0 1076    2    0    1    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    2    2    0    0    0    1    0    0   15\n",
      "     0   49]\n",
      " [   0    0   23  991    4    1    0    4    0    0    1    0    0    1\n",
      "     1    0    0    0    0    1   21    0    0    2    5   11   27   11\n",
      "     0   12]\n",
      " [   0    1   38    0  983    2    0   22    0    2   21    0    1    1\n",
      "     4    0    1   19    0    2    1    1    0    2    0    0    0   22\n",
      "     0   10]\n",
      " [   0    1    1    1    2  996    4   21    3   41   18    2   18    1\n",
      "    14    0    6    4    2    5    0    1    0    7    0    0    3    0\n",
      "     0    2]\n",
      " [  38   16    3    0    1    9  894    1    4    2    1    5   45    7\n",
      "     4    4   10    1    1    8    5    0    2   21    0    5   39    2\n",
      "     2    2]\n",
      " [   0    1    8    0    8   12    0 1003    1    8   32    0    2    0\n",
      "     3    1    5    0    1   14    4    4    2    1    0    4    2   11\n",
      "     0    5]\n",
      " [   2    8    1    2   24    3    8    2  883    5    9   13    4    5\n",
      "    30    1    8   17    3    7   15    4    8   17    4    5   16   29\n",
      "    13    3]\n",
      " [   0    0    6    0    1   26    1   55    1  910   69    4    1    0\n",
      "     7    1    0   12    0    2    2    8   19    3    0    1    6    2\n",
      "     7    0]\n",
      " [   0    1   12    0   10   14    0   79    2   18  914    1    0    0\n",
      "     8    6    0   34    0    5    3    5    0    5    0    0    6    4\n",
      "     0    2]\n",
      " [   1    5    7    0    1    5    8    4    2    5    3  942   13    0\n",
      "     0    1    1    3    1   71   10    2    1    1    1   13    6    5\n",
      "    21    2]\n",
      " [   9    9    5    1    1   15   12    3    1    5    5    3 1009    8\n",
      "    23    3    1    8    1   13    4    1    4    6    2    7    7    5\n",
      "     1    3]\n",
      " [   0    0    3    1    9    1    4    0    5    0    0    1    4  953\n",
      "    26    0    0   18    1   37    2    3    0    3    0    6    9   12\n",
      "     6    3]\n",
      " [   2    3    9    1   15   18    0   18    6    3   21    1    4    3\n",
      "   866    0    9   37    1   27    4    1    0   12    0    2    8   24\n",
      "     0    3]\n",
      " [   0    3    1    0    1    0    0    2    1    0    0    1    0    0\n",
      "     0 1128    0    1    0    1    1    1    1    0    0    3    3    0\n",
      "     1    0]\n",
      " [   2    0    3    0    2   12    3    6    2    0    1    1    0    4\n",
      "    29    0 1011    5    2   23    1    0    0   18    0    0    0    2\n",
      "     0    9]\n",
      " [   0    1   26    1   83    0    0    5    0    5   24    0    0    4\n",
      "    12    4    2  928    0    3    4    1    0    0    0    3    1    9\n",
      "     0    5]\n",
      " [   2    0    0    0    1   23   11    3   14    0    3    1   16    0\n",
      "     6    0    8    0 1019    2    7    3    1   18    0    4    2    2\n",
      "     0    0]\n",
      " [   0    0   14    2   16    1    0    4    2    2    7    6    2   20\n",
      "    14    0    5   14    0  958    2    6    1    2    1    3   18   20\n",
      "     4   32]\n",
      " [   0   10   17    5   12    0    0   11   10    8   20    3    3    1\n",
      "    11    8    2    6    0   39  881    4    8    2    2    9   44   23\n",
      "     1    9]\n",
      " [   5    8  127   17    9    2    1   27   15    9   11    8   10    0\n",
      "    11    0    0   14    1   21   14  577    6    0   14   53   21   66\n",
      "     0   53]\n",
      " [   0    1    1    0    0    0    0    2    1   19    2    0    1    0\n",
      "     3    0    0    4    0    5    1    0 1068    1    0    2    1    1\n",
      "     3    0]\n",
      " [  14    0    0    1    3   10    8    1    2    1   25    0    1    0\n",
      "    27    1   16    3    4    8    1    0    1 1013    1    2    7    1\n",
      "     4    3]\n",
      " [   1    0  114   18    0    0    0    0    0    1    0    1    1    0\n",
      "     2    0    0    9    0    1   20    6    0    0  814    1    0   73\n",
      "     1  109]\n",
      " [   3    5   12   16    7    0    1   17    3    0    2   10    4    1\n",
      "     2    0    1    6    0    5   15    7    2    1    5  959   12   30\n",
      "     2    4]\n",
      " [   4   16    8    3    2    0   11    0    0    1    3    5    2    3\n",
      "     4    8    0    5    0    6   14    0    0    7    1    6 1021    3\n",
      "     1    1]\n",
      " [   1    2  129    1   11    0    0    7    4    3    4    2    2    1\n",
      "    12    0    0   11    0   21    3    3    1    1   10    8    6  783\n",
      "     4  138]\n",
      " [   0    0    4    0    4    0    3    1    9    0    0   19    2    1\n",
      "     3    1    0    1    0   25    1    1    7    1    0    5   14    3\n",
      "  1061    3]\n",
      " [   0    0   41    2    5    0    0    2    2    0    2    0    0    0\n",
      "     0    0    0    0    0    0    7    0    0    0    2    0    5   21\n",
      "     2 1062]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      1134\n",
      "           1       0.91      0.84      0.87      1155\n",
      "           2       0.63      0.94      0.76      1148\n",
      "           3       0.93      0.89      0.91      1116\n",
      "           4       0.81      0.87      0.84      1133\n",
      "           5       0.86      0.86      0.86      1153\n",
      "           6       0.91      0.79      0.85      1132\n",
      "           7       0.77      0.89      0.82      1132\n",
      "           8       0.90      0.77      0.83      1149\n",
      "           9       0.87      0.80      0.83      1144\n",
      "          10       0.76      0.81      0.78      1129\n",
      "          11       0.91      0.83      0.87      1135\n",
      "          12       0.87      0.86      0.86      1175\n",
      "          13       0.94      0.86      0.90      1107\n",
      "          14       0.76      0.79      0.78      1098\n",
      "          15       0.96      0.98      0.97      1149\n",
      "          16       0.93      0.89      0.91      1136\n",
      "          17       0.80      0.83      0.81      1121\n",
      "          18       0.98      0.89      0.93      1146\n",
      "          19       0.72      0.83      0.77      1156\n",
      "          20       0.82      0.77      0.79      1149\n",
      "          21       0.90      0.52      0.66      1100\n",
      "          22       0.94      0.96      0.95      1116\n",
      "          23       0.87      0.87      0.87      1158\n",
      "          24       0.94      0.69      0.80      1172\n",
      "          25       0.86      0.85      0.85      1132\n",
      "          26       0.74      0.90      0.81      1135\n",
      "          27       0.66      0.67      0.67      1168\n",
      "          28       0.93      0.91      0.92      1169\n",
      "          29       0.70      0.92      0.79      1153\n",
      "\n",
      "    accuracy                           0.84     34200\n",
      "   macro avg       0.85      0.84      0.84     34200\n",
      "weighted avg       0.85      0.84      0.84     34200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 179/179 [00:24<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Metrics:\n",
      "Confusion Matrix:\n",
      "[[126   3   2   0   0   1   4   0   2   0   0   1   1   0   1   0   1   0\n",
      "    1   1   2   0   1   7   0   0   7   1   0   0]\n",
      " [  1 118   4   1   1   1   6   5   0   0   1   1   3   0   1   0   0   2\n",
      "    0   3   4   1   6   4   0   5  18   2   0   1]\n",
      " [  0   0 128   1   3   0   0   3   0   0   1   1   1   0   0   0   0   0\n",
      "    0   2   1   1   1   0   0   2   2   6   1  30]\n",
      " [  0   0  14 142   2   1   0   3   0   1   1   1   1   1   1   0   0   2\n",
      "    0   1   5   3   3   0   3   9   8  11   1   2]\n",
      " [  0   1   7   1 124   0   1   3   0   2   9   0   0   0   1   0   0  10\n",
      "    0   2   2   1   2   1   2   3   0  10   1   1]\n",
      " [  0   1   0   0   0 108   0   3   0   9   3   1   5   0   5   0   1   2\n",
      "    3   0   1   2   1   1   0   0   4   0   0   0]\n",
      " [ 14   3   0   1   0   4 117   0   1   0   1   3  18   0   1   0   0   0\n",
      "    3   4   2   1   1  14   0   5  10   3   0   3]\n",
      " [  0   0   6   1   3   1   0 117   1   5   9   0   0   0   3   0   1   2\n",
      "    1   8   2   2   2   1   0   2   0   0   0   1]\n",
      " [  4   5   1   3  12   2   5   3  84   1   7   5   2   3  11   0   3   4\n",
      "    0   1   7   1   2   3   2   6   3  13   4   1]\n",
      " [  1   0   4   1   2  11   2  12   2 102  25   4   3   0   1   0   1   0\n",
      "    0   4   7   4   6   2   0   1   4   0   0   0]\n",
      " [  1   0   7   1   4   6   0  17   2   6 133   0   0   1   4   2   0  13\n",
      "    0   4   1   1   0   4   1   0   5   1   0   3]\n",
      " [  0   2   5   2   0   1   7   0   3   4   2 106   4   0   0   0   0   0\n",
      "    0  16   7   2   1   1   0   8   1   3   7   3]\n",
      " [  2   1   1   1   3   6   3   0   1   5   5   4 104   5   7   0   1   5\n",
      "    0   4   0   0   2   6   1   4   7   3   3   2]\n",
      " [  1   1   3   2   1   2   4   3   2   0   0   1   2 108   8   0   2   8\n",
      "    0  16   3   0   2   3   0   4   9   8   8   2]\n",
      " [  1   2   1   0   5   6   0   7   4   3  12   1   9   3  80   1   4   4\n",
      "    0   9   2   3   0   4   0   1   4   9   0   2]\n",
      " [  0   0   2   0   0   0   0   0   0   1   0   0   0   0   0 189   0   0\n",
      "    0   0   0   0   0   0   0   0   2   0   0   0]\n",
      " [  2   1   1   1   3   4   1   1   0   1   2   0   0   3  13   0 137   2\n",
      "    0   9   0   0   0   4   0   0   2   2   0   4]\n",
      " [  0   1   6   0  29   1   0   2   2   0   5   0   2   3   7   0   0 115\n",
      "    0   4   1   1   0   0   1   1   3   7   0   7]\n",
      " [  1   0   0   0   1   9   4   1   1   1   2   0   6   1   2   0   1   0\n",
      "  137   0   0   1   0   3   0   1   2   0   0   1]\n",
      " [  0   0   7   0   3   1   1   6   1   2   4   1   3  13   8   0   2   8\n",
      "    0 102   7   1   0   1   0   2   5   9   7   8]\n",
      " [  4   4   3   6   4   2   1   4   7   2   8   2   3   0   4   3   1   2\n",
      "    0  11  81   2   2   0   0   5  15   9   0   5]\n",
      " [  0   3  29   5   2   1   0   7   1   5   8   5   2   0   4   0   0   3\n",
      "    1   8   9  56   0   0   2  13  12  16   1  11]\n",
      " [  1   2   2   0   0   0   0   1   0  11   1   1   1   0   2   0   0   0\n",
      "    0   2   0   1 181   1   0   2   1   1   1   0]\n",
      " [  6   2   0   0   2   2   2   2   0   1   7   1   5   3   9   0  10   1\n",
      "    1   5   1   2   0 117   0   0   7   0   1   1]\n",
      " [  0   1  27   4   1   0   4   0   0   0   0   1   1   0   1   0   0   2\n",
      "    0   2   4   1   0   0  60   1   3  22   1  22]\n",
      " [  2   3  11   7   3   0   1   4   1   0   1   2   3   1   2   0   1   3\n",
      "    0   8   6   5   2   0   2 110   4  13   0   5]\n",
      " [  2  18   4   4   2   1   5   2   0   1   5   1   1   4   4   1   0   3\n",
      "    0   8   4   1   1   1   0   4 112   6   2   1]\n",
      " [  0   2  33   1   4   1   0   4   1   0   3   0   1   2   4   0   0   5\n",
      "    0   5   6   1   0   0   3   1   6  85   2  27]\n",
      " [  1   1   3   0   1   0   4   1   7   1   1   1   0   1   3   3   3   1\n",
      "    0   6   2   0   1   1   0   1   3   0 119   0]\n",
      " [  0   1  18   0   0   1   0   2   0   0   0   0   1   0   1   0   0   3\n",
      "    0   3   2   1   0   0   1   0   0  11   0 154]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.78      0.76       162\n",
      "           1       0.67      0.62      0.65       189\n",
      "           2       0.39      0.70      0.50       184\n",
      "           3       0.77      0.66      0.71       216\n",
      "           4       0.58      0.67      0.62       184\n",
      "           5       0.62      0.72      0.67       150\n",
      "           6       0.68      0.56      0.61       209\n",
      "           7       0.55      0.70      0.61       168\n",
      "           8       0.68      0.42      0.52       198\n",
      "           9       0.62      0.51      0.56       199\n",
      "          10       0.52      0.61      0.56       217\n",
      "          11       0.74      0.57      0.64       185\n",
      "          12       0.57      0.56      0.57       186\n",
      "          13       0.71      0.53      0.61       203\n",
      "          14       0.43      0.45      0.44       177\n",
      "          15       0.95      0.97      0.96       194\n",
      "          16       0.81      0.71      0.76       193\n",
      "          17       0.57      0.58      0.58       198\n",
      "          18       0.93      0.78      0.85       175\n",
      "          19       0.41      0.50      0.45       202\n",
      "          20       0.48      0.43      0.45       190\n",
      "          21       0.59      0.27      0.37       204\n",
      "          22       0.83      0.85      0.84       212\n",
      "          23       0.65      0.62      0.64       188\n",
      "          24       0.77      0.38      0.51       158\n",
      "          25       0.58      0.55      0.56       200\n",
      "          26       0.43      0.57      0.49       198\n",
      "          27       0.34      0.43      0.38       197\n",
      "          28       0.75      0.72      0.73       165\n",
      "          29       0.52      0.77      0.62       199\n",
      "\n",
      "    accuracy                           0.61      5700\n",
      "   macro avg       0.63      0.61      0.61      5700\n",
      "weighted avg       0.63      0.61      0.61      5700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 1069/1069 [07:14<00:00,  2.46it/s, loss=0.648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished with loss: 0.6483045816421509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 1069/1069 [02:25<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Confusion Matrix:\n",
      "[[1098    0    0    0    2    1    2    0    1    0    0    0   10    0\n",
      "     2    0    3    0    2    0    1    0    0    5    5    2    0    0\n",
      "     0    0]\n",
      " [   1 1086    0    1    4    0    6    3    0    0    3    1    3    0\n",
      "     1    2    0    0    1    3    2    6    3    2    0    8   18    0\n",
      "     1    0]\n",
      " [   0    0 1119    0    2    0    0    1    1    0    0    1    0    0\n",
      "     1    0    0    0    0    0    0    0    0    0    2    0    0    6\n",
      "     0   15]\n",
      " [   1    0    1 1060    1    1    0    1    0    0    2    0    1    3\n",
      "     0    0    0    0    0    1    5    3    0    1    4   20    3    3\n",
      "     0    5]\n",
      " [   0    0    4    0 1090    0    0    3    1    1    6    0    0    2\n",
      "     1    0    1    1    0    0    2    1    0    2    0    2    0   13\n",
      "     0    3]\n",
      " [   1    2    0    0    1 1064    3    7    3   15   13    2    8    1\n",
      "     9    1    4    1    8    1    0    1    2    4    0    0    1    1\n",
      "     0    0]\n",
      " [  20    3    0    2    1    4 1029    0    7    1    1    2   17    3\n",
      "     0    0    7    1    0    0    1    0    4   10    0    8    7    2\n",
      "     2    0]\n",
      " [   1    0    1    1   10    7    0 1029    1   14   30    1    1    0\n",
      "     0    0    1    0    1    7    0    8    2    1    0    9    0    5\n",
      "     0    2]\n",
      " [   1    4    0    0    8    4    1    0 1043    2    2    7    2    5\n",
      "     1    0    4    3    4    2    4    3    3   12    1   12    6    6\n",
      "     9    0]\n",
      " [   1    1    1    0    1   19    2   12    1 1010   29    7    0    1\n",
      "     2    0    0    0    0    1    4    3   41    4    0    0    1    0\n",
      "     3    0]\n",
      " [   1    3    2    1   11    4    0    9    2   14 1063    0    0    1\n",
      "     1    0    2    2    0    2    2    2    1    3    0    0    2    1\n",
      "     0    0]\n",
      " [   2    6    1    0    0    1    5    1    1    3    1 1051    5    2\n",
      "     0    1    0    0    0   26    3    3    0    0    0    9    2    1\n",
      "    10    1]\n",
      " [   4    9    0    0    1   16   18    1    0    3    2    2 1073    5\n",
      "     9    2    1    0    4    0    0    2    3    1    0   17    0    0\n",
      "     2    0]\n",
      " [   1    1    0    0    2    0    0    0    1    0    1    0    1 1082\n",
      "     1    0    0    3    1    6    1    0    0    1    0    1    0    4\n",
      "     0    0]\n",
      " [   0    1    0    0    9    5    1    7   12    2   24    0    6   16\n",
      "   931    0    3    4    3   14    4    1    1   32    0    1    3   17\n",
      "     0    1]\n",
      " [   1    1    0    0    3    0    4    0    0    0    0    3    0    0\n",
      "     2 1122    0    1    1    0    1    1    0    0    0    3    1    0\n",
      "     5    0]\n",
      " [   0    0    0    0    1    8    1    2    2    2    4    1    2   11\n",
      "     8    0 1056    0    9    5    0    1    0   20    0    1    0    2\n",
      "     0    0]\n",
      " [   0    0    3    0  107    0    0    2    4    3   26    0    0    8\n",
      "    15    1    1  922    1    1    1    6    1    2    0    8    4    5\n",
      "     0    0]\n",
      " [   4    0    0    0    1    6    5    0    6    1    1    1    5    0\n",
      "     0    0    1    0 1107    0    3    0    0    3    0    1    0    1\n",
      "     0    0]\n",
      " [   0    1    0    0    7    1    0    1    1    0    7    4    1   27\n",
      "     1    0    3    1    0 1069    2    8    1    2    0    2    2    7\n",
      "     3    5]\n",
      " [   2    6    2    3    5    0    1    0    6    7    4    2    4    0\n",
      "     2    5    0    0    1    9 1034    5    4    0   14   12    9    8\n",
      "     0    4]\n",
      " [   1    2   14    7    5    0    0    0    7    6    6    5    3    4\n",
      "     1    0    1    3    5    4    4  920    8    0    6   65    4   14\n",
      "     0    5]\n",
      " [   1    1    0    1    0    0    0    1    0    2    0    0    1    0\n",
      "     0    0    0    0    0    0    0    0 1100    1    0    4    0    1\n",
      "     3    0]\n",
      " [   5    0    0    0    2    7    6    0    4    1    8    0    3    0\n",
      "     1    0    3    1    8    0    1    1    0 1099    1    3    2    0\n",
      "     2    0]\n",
      " [   0    1   16    6    0    0    1    0    0    0    2    0    2    1\n",
      "     3    0    0    0    0    1    4   16    0    0 1027   10    0   37\n",
      "     0   45]\n",
      " [   3    3    0    2    1    0    0    0    4    0    0    1    1    1\n",
      "     0    0    0    0    0    2    2   14    0    0    0 1098    0    0\n",
      "     0    0]\n",
      " [   5   15    0   13    1    2   17    1    3    9    2    3    2    0\n",
      "     0    7    0    1    0    3    6    4    0    4    0    9 1025    1\n",
      "     2    0]\n",
      " [   0    6   35    3    9    0    0    1    8    4    3    1    1    9\n",
      "     4    0    0    4    0    7    6    6    1    0    3   16    1  970\n",
      "     3   67]\n",
      " [   0    1    0    0    2    0    0    0    3    0    0    6    3    8\n",
      "     0    0    1    1    0    1    2    0    1    1    0    4    1    2\n",
      "  1132    0]\n",
      " [   0    1    6    0    3    0    0    0    0    1    1    0    1    0\n",
      "     0    0    1    1    0    5    1    2    0    1    2    0    1    5\n",
      "     1 1120]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1134\n",
      "           1       0.94      0.94      0.94      1155\n",
      "           2       0.93      0.97      0.95      1148\n",
      "           3       0.96      0.95      0.96      1116\n",
      "           4       0.84      0.96      0.90      1133\n",
      "           5       0.93      0.92      0.92      1153\n",
      "           6       0.93      0.91      0.92      1132\n",
      "           7       0.95      0.91      0.93      1132\n",
      "           8       0.93      0.91      0.92      1149\n",
      "           9       0.92      0.88      0.90      1144\n",
      "          10       0.86      0.94      0.90      1129\n",
      "          11       0.95      0.93      0.94      1135\n",
      "          12       0.93      0.91      0.92      1175\n",
      "          13       0.91      0.98      0.94      1107\n",
      "          14       0.93      0.85      0.89      1098\n",
      "          15       0.98      0.98      0.98      1149\n",
      "          16       0.97      0.93      0.95      1136\n",
      "          17       0.97      0.82      0.89      1121\n",
      "          18       0.96      0.97      0.96      1146\n",
      "          19       0.91      0.92      0.92      1156\n",
      "          20       0.94      0.90      0.92      1149\n",
      "          21       0.90      0.84      0.87      1100\n",
      "          22       0.94      0.99      0.96      1116\n",
      "          23       0.91      0.95      0.93      1158\n",
      "          24       0.96      0.88      0.92      1172\n",
      "          25       0.83      0.97      0.89      1132\n",
      "          26       0.94      0.90      0.92      1135\n",
      "          27       0.87      0.83      0.85      1168\n",
      "          28       0.96      0.97      0.96      1169\n",
      "          29       0.88      0.97      0.92      1153\n",
      "\n",
      "    accuracy                           0.92     34200\n",
      "   macro avg       0.93      0.92      0.92     34200\n",
      "weighted avg       0.93      0.92      0.92     34200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 179/179 [00:24<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Metrics:\n",
      "Confusion Matrix:\n",
      "[[118   3   0   0   0   1  12   0   2   0   0   0   4   0   1   0   2   0\n",
      "    2   1   3   1   1   6   1   0   3   0   0   1]\n",
      " [  3 134   0   0   2   0   6   2   1   1   0   3   4   1   0   0   0   0\n",
      "    0   1   4   1   4   4   0   4  10   3   0   1]\n",
      " [  0   3 111   1   8   0   0   1   1   1   2   3   2   1   1   0   0   1\n",
      "    0   0   2   7   1   0   3   4   0  12   3  16]\n",
      " [  0   0   5 135   3   1   0   2   2   1   1   1   1   3   1   0   0   1\n",
      "    0   0   6   8   3   1   6  18   5   9   2   1]\n",
      " [  1   0   1   1 132   1   1   1   3   2   8   0   1   2   2   0   0   5\n",
      "    0   2   1   1   2   0   2   3   0   8   2   2]\n",
      " [  0   0   0   0   0 103   1   2   2  10   3   2   9   2   2   0   1   0\n",
      "    5   0   1   2   2   2   0   0   1   0   0   0]\n",
      " [ 12   3   1   0   1   3 136   0   0   0   2   3  10   3   0   0   2   0\n",
      "    3   0   2   1   2  12   1   3   5   2   2   0]\n",
      " [  0   2   4   1   4   2   0 100   2   5  13   0   1   2   1   0   2   1\n",
      "    2   7   3   5   2   1   0   6   0   2   0   0]\n",
      " [  5   3   0   1   9   2   4   2 105   1   4   4   6   5   1   0   2   2\n",
      "    4   1   4   2   2   5   3   8   2   7   4   0]\n",
      " [  3   0   0   0   2  11   2  10   3 100  20   5   3   0   2   0   1   1\n",
      "    1   2   7   6  13   2   0   1   3   0   1   0]\n",
      " [  2   2   0   4  15   5   0  13   2  13 125   0   0   2   6   0   0   2\n",
      "    0   1   3   3   0   8   0   3   4   1   1   2]\n",
      " [  0   7   1   1   0   1   4   1   3   3   2 112   5   2   0   0   0   1\n",
      "    0   8   7   0   3   1   0  11   4   1   7   0]\n",
      " [  3   7   0   1   3   8   6   0   3   1   2   4 104   7   4   0   2   2\n",
      "    1   1   1   1   4   6   1   7   3   1   2   1]\n",
      " [  1   2   1   0   1   1   6   3   5   1   1   0   5 134   1   0   1   3\n",
      "    0  10   1   3   2   1   0   7   2   3   7   1]\n",
      " [  1   2   1   0   7   6   0   6   8   3  12   1   8  10  67   1   6   0\n",
      "    0  10   3   1   0   6   0   2   2  13   0   1]\n",
      " [  0   1   1   0   1   0   1   0   0   1   1   0   0   0   0 185   0   0\n",
      "    0   0   0   0   0   0   0   1   1   0   1   0]\n",
      " [  1   0   0   1   3   3   2   0   0   0   2   0   0   8   5   0 138   1\n",
      "    2  10   1   0   1  12   0   0   1   1   0   1]\n",
      " [  0   2   4   1  31   1   0   1   4   2   9   0   1   8   8   0   0  91\n",
      "    0   5   2   1   1   0   1   7   3  11   0   4]\n",
      " [  1   0   0   0   1   6   3   1   3   1   1   0   6   1   1   0   0   0\n",
      "  146   1   0   1   0   1   0   0   1   0   0   0]\n",
      " [  1   0   3   0   7   1   1   5   2   3   4   4   2  24   3   0   1   1\n",
      "    0  98   8   5   2   4   0   2   2  10   6   3]\n",
      " [  5   9   1   4   2   2   3   3   6   2   8   4   2   3   0   2   1   0\n",
      "    0   5  81   5   3   2   4  11   9   9   1   3]\n",
      " [  1   2  12   4   5   0   0   2   4   7   6   3   2   2   1   0   0   0\n",
      "    1   8   6  89   1   2   2  19   9  11   1   4]\n",
      " [  1   3   1   0   0   0   0   1   1   4   2   0   1   1   0   0   0   0\n",
      "    0   1   2   1 186   0   0   3   1   1   2   0]\n",
      " [  5   5   0   0   3   3   4   0   1   0   6   0   4   6   1   0   8   0\n",
      "    6   4   3   1   0 121   0   1   3   0   2   1]\n",
      " [  0   1  16   1   4   0   5   0   2   0   0   0   1   2   3   0   0   1\n",
      "    0   3   1   6   0   0  73   8   0  17   1  13]\n",
      " [  1   2   1   9   3   0   0   2   2   2   1   1   0   4   1   0   0   1\n",
      "    2   5   2  17   0   0   2 134   1   5   1   1]\n",
      " [  3  21   2   3   5   0   7   3   2   4   3   1   1   7   5   1   0   1\n",
      "    0   7   4   6   0   3   1   4  97   3   4   0]\n",
      " [  0   3  16   1   7   0   1   0   3   2   3   2   1   7   3   0   3   3\n",
      "    0   4   5  11   0   1   4   7   2  87   2  19]\n",
      " [  0   1   0   0   1   0   5   0   7   1   1   3   1  12   1   2   1   2\n",
      "    2   3   1   0   1   2   0   2   1   0 115   0]\n",
      " [  0   1  12   1   0   0   0   2   3   0   2   0   1   1   1   0   1   1\n",
      "    1   3   3   3   0   1   1   3   0  15   1 142]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72       162\n",
      "           1       0.61      0.71      0.66       189\n",
      "           2       0.57      0.60      0.59       184\n",
      "           3       0.79      0.62      0.70       216\n",
      "           4       0.51      0.72      0.59       184\n",
      "           5       0.64      0.69      0.66       150\n",
      "           6       0.65      0.65      0.65       209\n",
      "           7       0.61      0.60      0.60       168\n",
      "           8       0.58      0.53      0.55       198\n",
      "           9       0.58      0.50      0.54       199\n",
      "          10       0.51      0.58      0.54       217\n",
      "          11       0.72      0.61      0.66       185\n",
      "          12       0.56      0.56      0.56       186\n",
      "          13       0.52      0.66      0.58       203\n",
      "          14       0.55      0.38      0.45       177\n",
      "          15       0.97      0.95      0.96       194\n",
      "          16       0.80      0.72      0.76       193\n",
      "          17       0.75      0.46      0.57       198\n",
      "          18       0.82      0.83      0.83       175\n",
      "          19       0.49      0.49      0.49       202\n",
      "          20       0.49      0.43      0.45       190\n",
      "          21       0.47      0.44      0.45       204\n",
      "          22       0.79      0.88      0.83       212\n",
      "          23       0.59      0.64      0.62       188\n",
      "          24       0.70      0.46      0.56       158\n",
      "          25       0.48      0.67      0.56       200\n",
      "          26       0.55      0.49      0.52       198\n",
      "          27       0.38      0.44      0.41       197\n",
      "          28       0.68      0.70      0.69       165\n",
      "          29       0.65      0.71      0.68       199\n",
      "\n",
      "    accuracy                           0.61      5700\n",
      "   macro avg       0.62      0.61      0.61      5700\n",
      "weighted avg       0.62      0.61      0.61      5700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 1069/1069 [07:14<00:00,  2.46it/s, loss=0.397] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished with loss: 0.39719679951667786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics:  38%|███▊      | 408/1069 [00:55<01:30,  7.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27/3776094672.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;31m# Predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27/3776094672.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(ep)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# Calculate training and testing accuracy and metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0my_true_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Metrics:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mprint_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27/3776094672.py\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mall_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0m_limbo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0m_start_new_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bootstrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_active_limbo_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as AF\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "import re\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Custom dataset class for our image data\n",
    "class BookTitleDataset(Dataset):\n",
    "    def __init__(self, csv_file_x, csv_file_y, tokenizer, max_title_len, root_dir, transforms=None):\n",
    "        dataX = pd.read_csv(csv_file_x)\n",
    "        dataY = pd.read_csv(csv_file_y)\n",
    "        data = pd.merge(dataX, dataY, on='Id')\n",
    "        del dataX\n",
    "        del dataY\n",
    "        title = data.iloc[:, 2]\n",
    "        self.x = title\n",
    "        self.y = data.iloc[:, -1]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(self.y)\n",
    "        self.max_title_len = max_title_len\n",
    "        self.annotations = data\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        imgPath = os.path.join(self.root_dir, self.annotations.iloc[index, 1])\n",
    "        image = Image.open(imgPath)\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        x = self.tokenizer(x, padding='max_length', max_length=self.max_title_len, truncation=True,\n",
    "                           return_tensors=\"pt\")\n",
    "        return (image, x, y)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super(Bert, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        embeddings, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class Resnet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet18, self).__init__()\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "        self.model.avgpool = nn.Identity()\n",
    "        self.model.fc =  nn.Identity()\n",
    "        self.finalLayer = nn.Conv2d(512, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        ret = self.model(image)\n",
    "        ret = ret.reshape(ret.shape[0], 512, 7, 7)\n",
    "        ret = self.finalLayer(ret)\n",
    "        ret = ret.reshape(ret.shape[0], 768, 49).permute(0, 2, 1)\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "class Res_Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Res_Bert, self).__init__()\n",
    "        self.modelB = Bert()\n",
    "        self.modelA = Resnet18()\n",
    "        self.layerNorm = nn.LayerNorm(768)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.crossAttention = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.downsample = nn.AdaptiveAvgPool1d(1)\n",
    "        # self.classifier = nn.Linear(768, 30)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3), \n",
    "            nn.Linear(256, 30)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, mask, input_id):\n",
    "        x1 = self.modelA(image)\n",
    "        x2 = self.modelB(input_id, mask)\n",
    "        \n",
    "        query = x2\n",
    "        key = x1\n",
    "        value = x1\n",
    "        attention = self.apply_crossAttention(query, key, value)\n",
    "        x2 = attention[0] + x2\n",
    "        x2 = self.dropout(x2) \n",
    "        x2 = x2.permute(0, 2, 1)\n",
    "        x2 = self.downsample(x2)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        x2 = self.dropout(x2) \n",
    "        x = self.classifier(x2)\n",
    "        return x\n",
    "    \n",
    "    def apply_crossAttention(self, q, k, v):\n",
    "        # Shapes of q, k, v:\n",
    "        # q: [batch_size, 61, 768], k: [batch_size, 49, 768], v: [batch_size, 49, 768]\n",
    "        q_att = self.layerNorm(q)  # [32, 61, 768]\n",
    "        k_att = self.layerNorm(k)  # [32, 49, 768]\n",
    "        v_att = self.layerNorm(v)  # [32, 49, 768]\n",
    "        # Apply cross-attention block: q_att attends to k_att and v_att\n",
    "        att_out, att_map = self.crossAttention(q_att, k_att, v_att)\n",
    "        # Layer normalization on the output\n",
    "        att_out = self.layerNorm(att_out)  # [32, 61, 768]\n",
    "#         print(att_map.shape)\n",
    "        return att_out, att_map\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def train(ep=5):\n",
    "    # Training loop\n",
    "    for epoch in range(ep):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{ep}\")\n",
    "        for i, (img, embed, target) in enumerate(pbar):\n",
    "            img = img.to(device)\n",
    "            mask = embed['attention_mask'].to(device)\n",
    "            input_id = embed['input_ids'].squeeze(1).to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            out = model(img, mask, input_id)\n",
    "#             break\n",
    "#         break\n",
    "            loss = lossFunction(out, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        print(f'Epoch {epoch + 1} finished with loss: {loss.item()}')\n",
    "        # save(f'ver{epoch}')\n",
    "        save_checkpoint(model, optimizer, epoch, f'ver{epoch}')\n",
    "\n",
    "        # Calculate training and testing accuracy and metrics\n",
    "        y_true_train, y_pred_train = calculate_metrics(train_loader)\n",
    "        print(\"Training Metrics:\")\n",
    "        print_metrics(y_true_train, y_pred_train)\n",
    "\n",
    "        y_true_test, y_pred_test = calculate_metrics(test_loader)\n",
    "        print(\"Testing Metrics:\")\n",
    "        print_metrics(y_true_test, y_pred_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------HELPER FUNCTOINS-----------------------------------------------------\n",
    "def save_checkpoint(model, optimizer, epoch, file_name):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    torch.save(checkpoint, file_name)\n",
    "    print(f\"Checkpoint saved to {file_name}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(file_name, model, optimizer, resumeTraining = True):\n",
    "    checkpoint = torch.load(file_name)\n",
    "    # Load model and optimizer state_dict\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if(resumeTraining):\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # Load epoch value\n",
    "        epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {epoch}\")\n",
    "        return model, epoch, optimizer\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_metrics(loader):\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for img, embed, target in tqdm(loader, desc=\"Calculating Metrics\"):\n",
    "            img = img.to(device)\n",
    "            mask = embed['attention_mask'].to(device)\n",
    "            input_id = embed['input_ids'].squeeze(1).to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            out = model(img, mask, input_id)\n",
    "            all_preds.extend(torch.argmax(out, dim=1).cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    return np.array(all_targets), np.array(all_preds)\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "learning_rate = 5e-5\n",
    "batch_size = 32  \n",
    "\n",
    "# Calculated mean and std of the entire data separately\n",
    "mean = torch.tensor([0.5482, 0.5109, 0.4749])\n",
    "std = torch.tensor([0.2526, 0.2428, 0.2356])\n",
    "\n",
    "# Defining a custom transform pipeline\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomInvert(0.6),\n",
    "    transforms.RandomSolarize(0.6),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std), \n",
    "])\n",
    "\n",
    "training_data = BookTitleDataset(csv_file_x='/kaggle/input/col774-2022/train_x.csv',\n",
    "                                  csv_file_y='/kaggle/input/col774-2022/train_y.csv',\n",
    "                                  root_dir='/kaggle/input/col774-2022/images/images',\n",
    "                                  tokenizer=tokenizer, max_title_len=61, transforms=train_transform)\n",
    "\n",
    "testing_data = BookTitleDataset(csv_file_x='/kaggle/input/col774-2022/non_comp_test_x.csv',\n",
    "                                 csv_file_y='/kaggle/input/col774-2022/non_comp_test_y.csv',\n",
    "                                 root_dir='/kaggle/input/col774-2022/images/images',\n",
    "                                 tokenizer=tokenizer, max_title_len=61, transforms=test_transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=training_data, batch_size=batch_size, shuffle=True,  pin_memory=True, num_workers=os.cpu_count())\n",
    "test_loader = DataLoader(dataset=testing_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=os.cpu_count())    \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Res_Bert()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs.\")\n",
    "    model = nn.DataParallel(model)\n",
    "    # discriminator = nn.DataParallel(discriminator)\n",
    "\n",
    "# load('ver4')\n",
    "model = model.to(device)\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.1)\n",
    "\n",
    "train(ep=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_file_x, tokenizer, max_title_len, transforms=None):\n",
    "        data = pd.read_csv(csv_file_x)\n",
    "        title = data.iloc[:, 2]\n",
    "        self.x = [ tokenizer(i, padding='max_length', max_length=max_title_len, truncation=True, return_tensors=\"pt\") for i in title]\n",
    "        self.length = len(self.x)\n",
    "        self.annotations = data\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        imgPath = os.path.join(self.root_dir, self.annotations.iloc[index, 1])\n",
    "        ids = self.annotations.iloc[index, 0]\n",
    "        image = Image.open(imgPath)\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        x = self.x[index]\n",
    "        return (image, x, ids)\n",
    "    \n",
    "\n",
    "def prediction(model, loader, output_csv_path):\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_ids = []\n",
    "    with torch.no_grad():\n",
    "        for i, (img, embed, ids) in enumerate(loader):\n",
    "            img = img.to(device)\n",
    "            mask = embed['attention_mask'].to(device)\n",
    "            input_id = embed['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            outputs = model(img, mask, input_id)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "            \n",
    "            # Collect predictions\n",
    "            all_preds.extend(preds)\n",
    "            all_ids.extend(ids)\n",
    "            \n",
    "    # Save predictions to CSV\n",
    "    results = pd.DataFrame({'Id': all_ids, 'Genre': all_preds})\n",
    "    results.to_csv(output_csv_path, index=False)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Res_Bert()\n",
    "model = load_checkpoint('/kaggle/input/resbert_pretrained/pytorch/default/1/ver3', model, None, False)\n",
    "model.to(device)\n",
    "\n",
    "prediction_data = PredictionDataset(csv_file_x='/kaggle/input/col774-2022/comp_test_x.csv',\n",
    "                                root_dir='/kaggle/input/col774-2022/images/images',\n",
    "                                tokenizer=tokenizer, max_title_len=61, transforms=transform)\n",
    "\n",
    "prediction_loader = DataLoader(prediction_data, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "# Saving Predictions\n",
    "predictions(root_dir='/kaggle/input/col774-2022/images/images', path='/kaggle/input/col774-2022/comp_test_x.csv', fileName='file2')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 4467686,
     "sourceId": 40704,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 139389,
     "modelInstanceId": 116149,
     "sourceId": 137228,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30302,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
